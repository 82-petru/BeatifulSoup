{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beautiful Soup is a Python package for parsing HTML and XML documents. It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using the results module, we use the \"get\" function \n",
    "- argument to this function is the url \n",
    "- to make sure that the website is accessible, we can ensure that we obain a 200 ok response to indicate that the page is indeed present \n",
    "- we can also check the HTTP header of the website to verify indeeed taht we have access the corect page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url1 = \"https://www.google.com/\"\n",
    "result = requests.get(url1)\n",
    "result.status_code\n",
    "#print(result.headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now let us store the page content of the website accessed from requests to a variable and returns the source of that page "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = result.content \n",
    "#source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that we have the page sourced stored, we will use the BeautifulSoup module to parse and process the source, to do so, we created a BeautifulSoup object based on the source variable created above. This object allows us to extract certain types of information that might want to extract from this thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(source, \"lxml\")\n",
    "#soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now that the page source has been processed via BeautifulSoup we can access specific information directly from it.\n",
    "- let's say you want to see a list of all the links on the page. To do so we make use of  function \"find_all\" and pass in as argument the a tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = soup.find_all('a')\n",
    "#links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- perhaps you just want to extract the link with a specific string for example the text \"About\" on the page instead of every link, we can use the built-in function text to access the text content between  the <a> <a/> tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in links:\n",
    "    if \"About\" in link.text:\n",
    "        print(link)\n",
    "        print(link.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's obtain the links from the following website:**\n",
    "- Goal is to extract all the links on the page that point to the briefings and statements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = \"http://www.whitehouse.gov/briefings-statements/\"\n",
    "webpage = requests.get(url2)\n",
    "src = webpage.content\n",
    "soup1 = BeautifulSoup(src, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- create a list where to pupulate with links that we care about and then loop through all the links to find all h2 tags\n",
    "- find_all is a function that will return a list with all ones on the page, a_tag is just to find a single element to find a tag "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = []\n",
    "for h2_tag in soup.find_all(\"h2\"):\n",
    "    a_tag = h2_tag.find('a')\n",
    "    urls.append(a_tag.attrs['href'])\n",
    "urls    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
